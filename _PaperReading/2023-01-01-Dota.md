---
title: 'DOTA'
date: 2023-01-01
permalink: /PaperReading/2023/01/01/DOTA
tags:
  - spatial accelerator
  - transformer
  - dataflow opt
---

Authors: Qu, Zheng, Liu Liu, Fengbin Tu, Zhaodong Chen, Yufei Ding, and Yuan Xie



  

# Intro

无需多言，Transformer已经是深度学习领域最热门的模型之一，在各种场景应用（NLP，CV，etc.）中都占据SOTA地位。自注意力机制是Transformer的核心，可以用注意力图来表示。

但是Transformer也有他自己的缺点，包括attention机制的**平方计算复杂度**和**存储需求**。



本文的novelty:

* 提出优化Transformer模型的访存和计算的DSA架构
* 采用co-design的方式，提出一种优化了的轻量级Detector，用于检测和放弃注意力图上的弱连接。
* 在上述工作基础上完成端到端系统设计，构成**DOTA**

# Problems

TBD

# Weak Attentions Detection

TBD

# System Design

TBD

# Comment

TBD



